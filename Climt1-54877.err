2025-02-08 10:59:13,397 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.2:44683'
2025-02-08 10:59:13,400 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.2:40495'
2025-02-08 10:59:13,405 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.2:40709'
2025-02-08 10:59:13,415 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.2:45825'
2025-02-08 10:59:13,418 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.2:35899'
2025-02-08 10:59:15,349 - distributed.worker - INFO -       Start worker at:     tcp://172.26.1.2:37179
2025-02-08 10:59:15,349 - distributed.worker - INFO -          Listening to:     tcp://172.26.1.2:37179
2025-02-08 10:59:15,349 - distributed.worker - INFO -           Worker name:           SLURMCluster-8-0
2025-02-08 10:59:15,349 - distributed.worker - INFO -          dashboard at:           172.26.1.2:34471
2025-02-08 10:59:15,349 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:15,349 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,349 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:15,349 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:15,349 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a1b__7sh
2025-02-08 10:59:15,349 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,356 - distributed.worker - INFO -       Start worker at:     tcp://172.26.1.2:41177
2025-02-08 10:59:15,356 - distributed.worker - INFO -          Listening to:     tcp://172.26.1.2:41177
2025-02-08 10:59:15,356 - distributed.worker - INFO -           Worker name:           SLURMCluster-8-3
2025-02-08 10:59:15,356 - distributed.worker - INFO -          dashboard at:           172.26.1.2:37881
2025-02-08 10:59:15,356 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:15,356 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,356 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:15,356 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:15,356 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y7eckvt_
2025-02-08 10:59:15,356 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,374 - distributed.worker - INFO -       Start worker at:     tcp://172.26.1.2:38577
2025-02-08 10:59:15,374 - distributed.worker - INFO -          Listening to:     tcp://172.26.1.2:38577
2025-02-08 10:59:15,374 - distributed.worker - INFO -           Worker name:           SLURMCluster-8-4
2025-02-08 10:59:15,374 - distributed.worker - INFO -          dashboard at:           172.26.1.2:43379
2025-02-08 10:59:15,374 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:15,374 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,374 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:15,374 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:15,374 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nimujqzh
2025-02-08 10:59:15,374 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,449 - distributed.worker - INFO -       Start worker at:     tcp://172.26.1.2:41355
2025-02-08 10:59:15,449 - distributed.worker - INFO -          Listening to:     tcp://172.26.1.2:41355
2025-02-08 10:59:15,449 - distributed.worker - INFO -           Worker name:           SLURMCluster-8-2
2025-02-08 10:59:15,449 - distributed.worker - INFO -          dashboard at:           172.26.1.2:37479
2025-02-08 10:59:15,449 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:15,449 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,449 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:15,449 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:15,450 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uto3yhic
2025-02-08 10:59:15,450 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,485 - distributed.worker - INFO -       Start worker at:     tcp://172.26.1.2:43239
2025-02-08 10:59:15,485 - distributed.worker - INFO -          Listening to:     tcp://172.26.1.2:43239
2025-02-08 10:59:15,485 - distributed.worker - INFO -           Worker name:           SLURMCluster-8-1
2025-02-08 10:59:15,485 - distributed.worker - INFO -          dashboard at:           172.26.1.2:34159
2025-02-08 10:59:15,485 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:15,485 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,485 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:15,485 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:15,485 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4w2m6gn4
2025-02-08 10:59:15,485 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,243 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:16,244 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,244 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,244 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:16,244 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
2025-02-08 10:59:16,245 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,245 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,245 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:16,246 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,246 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,246 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:16,247 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,247 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,247 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
2025-02-08 10:59:16,247 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
2025-02-08 10:59:16,247 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
2025-02-08 10:59:16,248 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:16,249 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,249 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,249 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
slurmstepd-node02: error: *** JOB 54877 ON node02 CANCELLED AT 2025-02-08T11:14:38 DUE TO TIME LIMIT ***
