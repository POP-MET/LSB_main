2025-02-08 11:01:13,521 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.15:41391'
2025-02-08 11:01:13,536 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.15:38697'
2025-02-08 11:01:13,553 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.15:37039'
2025-02-08 11:01:13,568 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.15:40763'
2025-02-08 11:01:13,590 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.15:38943'
2025-02-08 11:01:16,023 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.15:37115
2025-02-08 11:01:16,032 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.15:37115
2025-02-08 11:01:16,032 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-0
2025-02-08 11:01:16,032 - distributed.worker - INFO -          dashboard at:          172.26.1.15:41793
2025-02-08 11:01:16,032 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:44245
2025-02-08 11:01:16,032 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,032 - distributed.worker - INFO -               Threads:                          2
2025-02-08 11:01:16,032 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 11:01:16,032 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c3s5zqte
2025-02-08 11:01:16,032 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,082 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.15:44969
2025-02-08 11:01:16,082 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.15:44969
2025-02-08 11:01:16,082 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-4
2025-02-08 11:01:16,083 - distributed.worker - INFO -          dashboard at:          172.26.1.15:38133
2025-02-08 11:01:16,083 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:44245
2025-02-08 11:01:16,083 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,083 - distributed.worker - INFO -               Threads:                          2
2025-02-08 11:01:16,083 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 11:01:16,083 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-220odu7m
2025-02-08 11:01:16,084 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,189 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.15:45939
2025-02-08 11:01:16,189 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.15:45939
2025-02-08 11:01:16,189 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-2
2025-02-08 11:01:16,189 - distributed.worker - INFO -          dashboard at:          172.26.1.15:34721
2025-02-08 11:01:16,189 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:44245
2025-02-08 11:01:16,189 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,189 - distributed.worker - INFO -               Threads:                          2
2025-02-08 11:01:16,189 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 11:01:16,189 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ipbt6rla
2025-02-08 11:01:16,189 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,427 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.15:35917
2025-02-08 11:01:16,427 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.15:35917
2025-02-08 11:01:16,427 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-1
2025-02-08 11:01:16,427 - distributed.worker - INFO -          dashboard at:          172.26.1.15:36973
2025-02-08 11:01:16,428 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:44245
2025-02-08 11:01:16,428 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,428 - distributed.worker - INFO -               Threads:                          2
2025-02-08 11:01:16,428 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 11:01:16,428 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tm5nl1go
2025-02-08 11:01:16,428 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,432 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.15:35837
2025-02-08 11:01:16,433 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.15:35837
2025-02-08 11:01:16,433 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-3
2025-02-08 11:01:16,433 - distributed.worker - INFO -          dashboard at:          172.26.1.15:32893
2025-02-08 11:01:16,433 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:44245
2025-02-08 11:01:16,433 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,433 - distributed.worker - INFO -               Threads:                          2
2025-02-08 11:01:16,433 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 11:01:16,433 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t8btx4z7
2025-02-08 11:01:16,433 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,945 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 11:01:16,946 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:44245
2025-02-08 11:01:16,946 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,969 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 11:01:16,970 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:44245
2025-02-08 11:01:16,970 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,973 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:44245
2025-02-08 11:01:16,979 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:44245
2025-02-08 11:01:17,032 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 11:01:17,033 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:44245
2025-02-08 11:01:17,033 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:17,034 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:44245
2025-02-08 11:01:17,071 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 11:01:17,072 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:44245
2025-02-08 11:01:17,072 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:17,074 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:44245
2025-02-08 11:01:17,173 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 11:01:17,174 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:44245
2025-02-08 11:01:17,174 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:17,175 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:44245
slurmstepd-node15: error: *** JOB 54887 ON node15 CANCELLED AT 2025-02-08T11:16:38 DUE TO TIME LIMIT ***
