{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    job_name=\"Climt1\",          # --job-name\n",
    "    cores=2,                     # Number of cores per task (adjust if needed)\n",
    "    processes=5,                 # One process per task\n",
    "    memory=\"100GB\",               # --mem\n",
    "    walltime=\"00:15:00\",         # --time\n",
    "    queue=\"med\",               # --partition\n",
    "    log_directory=\".\",           # Logs will be saved to the current directory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "cluster.scale(jobs=10)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the cluster\n",
    "import dask.array as da\n",
    "\n",
    "x = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "\n",
    "(x + x.T).mean().compute()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dft(in_dir,arm_data=True):\n",
    "    files = glob.glob(in_dir+os.sep+'*.nc')\n",
    "    if len(files) == 0:\n",
    "        files = glob.glob(in_dir+os.sep+'*.cdf')\n",
    "        if len(files) == 0:\n",
    "            raise Exception('No files Found')\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    dft = pd.DataFrame(files,columns=['filepath'])\n",
    "    dft['filename'] = dft['filepath'].str.split(os.sep).str[-1]\n",
    "    \n",
    "    if arm_data:\n",
    "        dft['datetime'] = pd.to_datetime(dft['filename'].str.split(\n",
    "            '.',\n",
    "            expand=True).iloc[:, 2] + dft['filename'].str.split(\n",
    "                '.',  expand=True).iloc[:, 3],format='%Y%m%d%H%M%S')\n",
    "    return dft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_era = '/home1/nalex2023/Datasets/era5_manus/'\n",
    "\n",
    "dft = create_dft(in_era,arm_data=False)\n",
    "\n",
    "dft['datetime'] = dft['filename'].str.split('_').str[-1].str[:6]\n",
    "\n",
    "dft['datetime'] = pd.to_datetime(dft['datetime'],format='%Y%m')\n",
    "\n",
    "dft_sort = dft.sort_values('datetime')\n",
    "\n",
    "dft_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "def prepro(ds):\n",
    "    ds = ds.sel(pressure_level=slice(1000,800))[['u','v']]\n",
    "    return ds\n",
    "\"\"\"\n",
    "def prepro(ds):\n",
    "    ds = ds.sel(pressure_level=1000)['u']\n",
    "    return ds\n",
    "\n",
    "\n",
    "dset = xr.open_mfdataset(dft_sort['filepath'],preprocess=prepro,combine='by_coords',parallel=True,chunks='auto')\n",
    "\n",
    "#hourly_summary = dset[['u','v']].groupby('valid_time.hour').mean()\n",
    "\n",
    "hourly_summary = dset['u'].groupby('valid_time.hour').mean()\n",
    "\n",
    "#seasonal = dset[['u','v']].groupby('valid_time.season').apply(lambda x: x.groupby('valid_time.hour').mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_summary\n",
    "#seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_summary['speed'] = np.sqrt((hourly_summary['u'] * hourly_summary['u']) + (hourly_summary['v'] * hourly_summary['v']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_loc = hourly_summary.compute()\n",
    "\n",
    "\n",
    "\n",
    "#final_data_loc = seasonal.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '/home1/nalex2023/Datasets/LSB_OUTS/'\n",
    "\n",
    "final_data_loc.to_netcdf(out_dir+'era5_maritime_diurnal.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_fol = '/home1/nalex2023/Datasets/sst_datasets'\n",
    "\n",
    "sst_files = glob.glob(sst_fol + '/*.nc')\n",
    "\n",
    "sst_files_df = pd.DataFrame(sst_files,columns=['filepath'])\n",
    "\n",
    "sst_files_df['filename'] = sst_files_df['filepath'].str.split(os.sep).str[-1]\n",
    "\n",
    "sst_files_df['datetime'] = sst_files_df['filename'].str.split('.').str[-2]\n",
    "\n",
    "test = xr.open_dataset(sst_files_df.iloc[22]['filepath'])\n",
    "sst_files_df['datetime'] = sst_files_df['datetime'].str.replace(\" (1)\",'')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_sst(ds):\n",
    "    ds_sel = ds.sel(latitude=slice(-5,5),longitude=slice(141,151))\n",
    "    return ds_sel\n",
    "\n",
    "test = xr.open_dataset(sst_files_df.iloc[0]['filepath'])\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
