2025-02-08 10:59:13,598 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.14:39223'
2025-02-08 10:59:13,614 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.14:34441'
2025-02-08 10:59:13,618 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.14:44619'
2025-02-08 10:59:13,629 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.14:33759'
2025-02-08 10:59:13,645 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.14:40275'
2025-02-08 10:59:15,996 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.14:41641
2025-02-08 10:59:15,997 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.14:41641
2025-02-08 10:59:15,997 - distributed.worker - INFO -           Worker name:           SLURMCluster-6-4
2025-02-08 10:59:15,997 - distributed.worker - INFO -          dashboard at:          172.26.1.14:38811
2025-02-08 10:59:15,997 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:15,997 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,997 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:15,997 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:15,997 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-canaedkw
2025-02-08 10:59:15,997 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,146 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.14:46535
2025-02-08 10:59:16,152 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.14:46535
2025-02-08 10:59:16,152 - distributed.worker - INFO -           Worker name:           SLURMCluster-6-0
2025-02-08 10:59:16,152 - distributed.worker - INFO -          dashboard at:          172.26.1.14:33595
2025-02-08 10:59:16,152 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,153 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,153 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:16,160 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:16,160 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0d3pode_
2025-02-08 10:59:16,160 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,180 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.14:40133
2025-02-08 10:59:16,183 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.14:40133
2025-02-08 10:59:16,183 - distributed.worker - INFO -           Worker name:           SLURMCluster-6-1
2025-02-08 10:59:16,183 - distributed.worker - INFO -          dashboard at:          172.26.1.14:33593
2025-02-08 10:59:16,183 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,183 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,183 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:16,183 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:16,183 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-is_wo_hc
2025-02-08 10:59:16,183 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,474 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.14:38975
2025-02-08 10:59:16,474 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.14:38975
2025-02-08 10:59:16,474 - distributed.worker - INFO -           Worker name:           SLURMCluster-6-3
2025-02-08 10:59:16,474 - distributed.worker - INFO -          dashboard at:          172.26.1.14:38259
2025-02-08 10:59:16,474 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,474 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,475 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:16,475 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:16,475 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fj3zm3xd
2025-02-08 10:59:16,475 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,637 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.14:36389
2025-02-08 10:59:16,638 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.14:36389
2025-02-08 10:59:16,639 - distributed.worker - INFO -           Worker name:           SLURMCluster-6-2
2025-02-08 10:59:16,640 - distributed.worker - INFO -          dashboard at:          172.26.1.14:37391
2025-02-08 10:59:16,640 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,640 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,640 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:16,641 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:16,641 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xzpotc5n
2025-02-08 10:59:16,641 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:17,013 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:17,014 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:17,014 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:17,023 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
2025-02-08 10:59:17,101 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:17,102 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:17,102 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:17,115 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
2025-02-08 10:59:17,143 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:17,143 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:17,144 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:17,154 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
2025-02-08 10:59:17,162 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:17,163 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:17,163 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:17,165 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
2025-02-08 10:59:17,297 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:17,298 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:17,298 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:17,299 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
slurmstepd-node14: error: *** JOB 54884 ON node14 CANCELLED AT 2025-02-08T11:14:38 DUE TO TIME LIMIT ***
