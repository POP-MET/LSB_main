2025-02-08 10:59:13,597 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.14:34193'
2025-02-08 10:59:13,612 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.14:42073'
2025-02-08 10:59:13,618 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.14:37465'
2025-02-08 10:59:13,625 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.14:42591'
2025-02-08 10:59:13,645 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.14:41951'
2025-02-08 10:59:16,139 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.14:41907
2025-02-08 10:59:16,140 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.14:41907
2025-02-08 10:59:16,141 - distributed.worker - INFO -           Worker name:           SLURMCluster-2-4
2025-02-08 10:59:16,141 - distributed.worker - INFO -          dashboard at:          172.26.1.14:34485
2025-02-08 10:59:16,141 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,141 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,141 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:16,142 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:16,142 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k8br9b9j
2025-02-08 10:59:16,142 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,142 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.14:38987
2025-02-08 10:59:16,143 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.14:38987
2025-02-08 10:59:16,143 - distributed.worker - INFO -           Worker name:           SLURMCluster-2-1
2025-02-08 10:59:16,143 - distributed.worker - INFO -          dashboard at:          172.26.1.14:45445
2025-02-08 10:59:16,144 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,144 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,144 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:16,144 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:16,144 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-111ud_70
2025-02-08 10:59:16,144 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,208 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.14:38835
2025-02-08 10:59:16,209 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.14:38835
2025-02-08 10:59:16,209 - distributed.worker - INFO -           Worker name:           SLURMCluster-2-2
2025-02-08 10:59:16,209 - distributed.worker - INFO -          dashboard at:          172.26.1.14:37921
2025-02-08 10:59:16,209 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,209 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,209 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:16,209 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:16,209 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-09ystpqu
2025-02-08 10:59:16,210 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,248 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.14:38779
2025-02-08 10:59:16,249 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.14:38779
2025-02-08 10:59:16,249 - distributed.worker - INFO -           Worker name:           SLURMCluster-2-3
2025-02-08 10:59:16,249 - distributed.worker - INFO -          dashboard at:          172.26.1.14:36771
2025-02-08 10:59:16,249 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,249 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,249 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:16,249 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:16,249 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2uzr7u0s
2025-02-08 10:59:16,249 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,582 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.14:42663
2025-02-08 10:59:16,583 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.14:42663
2025-02-08 10:59:16,583 - distributed.worker - INFO -           Worker name:           SLURMCluster-2-0
2025-02-08 10:59:16,583 - distributed.worker - INFO -          dashboard at:          172.26.1.14:36275
2025-02-08 10:59:16,583 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,583 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,583 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:16,583 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:16,583 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iu1g8bgt
2025-02-08 10:59:16,583 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:17,011 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:17,011 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:17,011 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:17,022 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
2025-02-08 10:59:17,100 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:17,100 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:17,100 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:17,108 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
2025-02-08 10:59:17,180 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:17,181 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:17,181 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:17,183 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
2025-02-08 10:59:17,184 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:17,185 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:17,185 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:17,186 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
2025-02-08 10:59:17,220 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:17,221 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:17,221 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:17,222 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
slurmstepd-node14: error: *** JOB 54885 ON node14 CANCELLED AT 2025-02-08T11:14:38 DUE TO TIME LIMIT ***
