2025-02-05 15:32:10,014 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.14:34281'
2025-02-05 15:32:10,073 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.14:35975'
2025-02-05 15:32:10,109 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.14:45313'
2025-02-05 15:32:10,145 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.14:39927'
2025-02-05 15:32:10,181 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.14:39849'
2025-02-05 15:32:12,242 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.14:35673
2025-02-05 15:32:12,242 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.14:35673
2025-02-05 15:32:12,242 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-0
2025-02-05 15:32:12,242 - distributed.worker - INFO -          dashboard at:          172.26.1.14:35819
2025-02-05 15:32:12,242 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:40259
2025-02-05 15:32:12,242 - distributed.worker - INFO - -------------------------------------------------
2025-02-05 15:32:12,242 - distributed.worker - INFO -               Threads:                          2
2025-02-05 15:32:12,242 - distributed.worker - INFO -                Memory:                   7.45 GiB
2025-02-05 15:32:12,242 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sat6yas0
2025-02-05 15:32:12,242 - distributed.worker - INFO - -------------------------------------------------
2025-02-05 15:32:12,266 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.14:41555
2025-02-05 15:32:12,266 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.14:41555
2025-02-05 15:32:12,266 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-1
2025-02-05 15:32:12,266 - distributed.worker - INFO -          dashboard at:          172.26.1.14:38617
2025-02-05 15:32:12,266 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:40259
2025-02-05 15:32:12,266 - distributed.worker - INFO - -------------------------------------------------
2025-02-05 15:32:12,266 - distributed.worker - INFO -               Threads:                          2
2025-02-05 15:32:12,266 - distributed.worker - INFO -                Memory:                   7.45 GiB
2025-02-05 15:32:12,266 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-93zzhle1
2025-02-05 15:32:12,266 - distributed.worker - INFO - -------------------------------------------------
2025-02-05 15:32:12,396 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.14:38507
2025-02-05 15:32:12,396 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.14:38507
2025-02-05 15:32:12,396 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-4
2025-02-05 15:32:12,397 - distributed.worker - INFO -          dashboard at:          172.26.1.14:46467
2025-02-05 15:32:12,397 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:40259
2025-02-05 15:32:12,397 - distributed.worker - INFO - -------------------------------------------------
2025-02-05 15:32:12,397 - distributed.worker - INFO -               Threads:                          2
2025-02-05 15:32:12,397 - distributed.worker - INFO -                Memory:                   7.45 GiB
2025-02-05 15:32:12,397 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wgriuakx
2025-02-05 15:32:12,397 - distributed.worker - INFO - -------------------------------------------------
2025-02-05 15:32:12,567 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.14:45315
2025-02-05 15:32:12,567 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.14:45315
2025-02-05 15:32:12,567 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-2
2025-02-05 15:32:12,567 - distributed.worker - INFO -          dashboard at:          172.26.1.14:43821
2025-02-05 15:32:12,567 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:40259
2025-02-05 15:32:12,567 - distributed.worker - INFO - -------------------------------------------------
2025-02-05 15:32:12,567 - distributed.worker - INFO -               Threads:                          2
2025-02-05 15:32:12,568 - distributed.worker - INFO -                Memory:                   7.45 GiB
2025-02-05 15:32:12,568 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5au0y48y
2025-02-05 15:32:12,568 - distributed.worker - INFO - -------------------------------------------------
2025-02-05 15:32:12,609 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.14:39851
2025-02-05 15:32:12,610 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.14:39851
2025-02-05 15:32:12,610 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-3
2025-02-05 15:32:12,610 - distributed.worker - INFO -          dashboard at:          172.26.1.14:39439
2025-02-05 15:32:12,610 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:40259
2025-02-05 15:32:12,610 - distributed.worker - INFO - -------------------------------------------------
2025-02-05 15:32:12,610 - distributed.worker - INFO -               Threads:                          2
2025-02-05 15:32:12,610 - distributed.worker - INFO -                Memory:                   7.45 GiB
2025-02-05 15:32:12,610 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ir8s39iw
2025-02-05 15:32:12,610 - distributed.worker - INFO - -------------------------------------------------
2025-02-05 15:32:12,939 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-05 15:32:12,939 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:40259
2025-02-05 15:32:12,939 - distributed.worker - INFO - -------------------------------------------------
2025-02-05 15:32:12,941 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-05 15:32:12,942 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:40259
2025-02-05 15:32:12,942 - distributed.worker - INFO - -------------------------------------------------
2025-02-05 15:32:12,949 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:40259
2025-02-05 15:32:12,950 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:40259
2025-02-05 15:32:13,129 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-05 15:32:13,130 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:40259
2025-02-05 15:32:13,130 - distributed.worker - INFO - -------------------------------------------------
2025-02-05 15:32:13,141 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:40259
2025-02-05 15:32:13,271 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-05 15:32:13,272 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:40259
2025-02-05 15:32:13,272 - distributed.worker - INFO - -------------------------------------------------
2025-02-05 15:32:13,281 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:40259
2025-02-05 15:32:13,292 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-05 15:32:13,293 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:40259
2025-02-05 15:32:13,293 - distributed.worker - INFO - -------------------------------------------------
2025-02-05 15:32:13,302 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:40259
slurmstepd-node14: error: *** JOB 53684 ON node14 CANCELLED AT 2025-02-05T15:47:29 DUE TO TIME LIMIT ***
