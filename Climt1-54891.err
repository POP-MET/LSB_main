2025-02-08 11:01:13,607 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.16:45927'
2025-02-08 11:01:13,621 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.16:45857'
2025-02-08 11:01:13,628 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.16:40253'
2025-02-08 11:01:13,634 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.16:40665'
2025-02-08 11:01:13,652 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.16:33621'
2025-02-08 11:01:16,178 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.16:37013
2025-02-08 11:01:16,178 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.16:37013
2025-02-08 11:01:16,178 - distributed.worker - INFO -           Worker name:           SLURMCluster-3-3
2025-02-08 11:01:16,178 - distributed.worker - INFO -          dashboard at:          172.26.1.16:45395
2025-02-08 11:01:16,178 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:44245
2025-02-08 11:01:16,178 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,178 - distributed.worker - INFO -               Threads:                          2
2025-02-08 11:01:16,178 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 11:01:16,178 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p_r9c5q4
2025-02-08 11:01:16,178 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,183 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.16:45245
2025-02-08 11:01:16,183 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.16:45245
2025-02-08 11:01:16,184 - distributed.worker - INFO -           Worker name:           SLURMCluster-3-1
2025-02-08 11:01:16,184 - distributed.worker - INFO -          dashboard at:          172.26.1.16:41251
2025-02-08 11:01:16,184 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:44245
2025-02-08 11:01:16,189 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,189 - distributed.worker - INFO -               Threads:                          2
2025-02-08 11:01:16,189 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 11:01:16,189 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2trfcg5j
2025-02-08 11:01:16,189 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,230 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.16:36545
2025-02-08 11:01:16,230 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.16:36545
2025-02-08 11:01:16,230 - distributed.worker - INFO -           Worker name:           SLURMCluster-3-2
2025-02-08 11:01:16,230 - distributed.worker - INFO -          dashboard at:          172.26.1.16:35377
2025-02-08 11:01:16,230 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:44245
2025-02-08 11:01:16,230 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,230 - distributed.worker - INFO -               Threads:                          2
2025-02-08 11:01:16,231 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 11:01:16,231 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v0ydy6l7
2025-02-08 11:01:16,231 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,363 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.16:44327
2025-02-08 11:01:16,364 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.16:44327
2025-02-08 11:01:16,364 - distributed.worker - INFO -           Worker name:           SLURMCluster-3-0
2025-02-08 11:01:16,364 - distributed.worker - INFO -          dashboard at:          172.26.1.16:34067
2025-02-08 11:01:16,364 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:44245
2025-02-08 11:01:16,364 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,364 - distributed.worker - INFO -               Threads:                          2
2025-02-08 11:01:16,364 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 11:01:16,364 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0dupq1v2
2025-02-08 11:01:16,364 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,642 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.16:39983
2025-02-08 11:01:16,642 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.16:39983
2025-02-08 11:01:16,643 - distributed.worker - INFO -           Worker name:           SLURMCluster-3-4
2025-02-08 11:01:16,643 - distributed.worker - INFO -          dashboard at:          172.26.1.16:37181
2025-02-08 11:01:16,643 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:44245
2025-02-08 11:01:16,643 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:16,643 - distributed.worker - INFO -               Threads:                          2
2025-02-08 11:01:16,643 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 11:01:16,643 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xyrbj9vb
2025-02-08 11:01:16,643 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:17,118 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 11:01:17,119 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:44245
2025-02-08 11:01:17,119 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:17,120 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 11:01:17,120 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:44245
2025-02-08 11:01:17,120 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:17,131 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:44245
2025-02-08 11:01:17,139 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:44245
2025-02-08 11:01:17,151 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 11:01:17,152 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:44245
2025-02-08 11:01:17,152 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:17,156 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:44245
2025-02-08 11:01:17,174 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 11:01:17,175 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:44245
2025-02-08 11:01:17,175 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:17,176 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:44245
2025-02-08 11:01:17,277 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 11:01:17,277 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:44245
2025-02-08 11:01:17,277 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 11:01:17,278 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:44245
slurmstepd-node16: error: *** JOB 54891 ON node16 CANCELLED AT 2025-02-08T11:16:38 DUE TO TIME LIMIT ***
