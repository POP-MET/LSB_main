2025-02-08 10:59:13,389 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.2:44343'
2025-02-08 10:59:13,405 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.2:36973'
2025-02-08 10:59:13,414 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.2:37017'
2025-02-08 10:59:13,416 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.2:38501'
2025-02-08 10:59:13,422 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.2:32851'
2025-02-08 10:59:15,325 - distributed.worker - INFO -       Start worker at:     tcp://172.26.1.2:44601
2025-02-08 10:59:15,325 - distributed.worker - INFO -          Listening to:     tcp://172.26.1.2:44601
2025-02-08 10:59:15,325 - distributed.worker - INFO -           Worker name:           SLURMCluster-4-2
2025-02-08 10:59:15,325 - distributed.worker - INFO -          dashboard at:           172.26.1.2:45291
2025-02-08 10:59:15,325 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:15,325 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,325 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:15,325 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:15,325 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ug74e2l8
2025-02-08 10:59:15,325 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,407 - distributed.worker - INFO -       Start worker at:     tcp://172.26.1.2:44559
2025-02-08 10:59:15,408 - distributed.worker - INFO -          Listening to:     tcp://172.26.1.2:44559
2025-02-08 10:59:15,408 - distributed.worker - INFO -           Worker name:           SLURMCluster-4-0
2025-02-08 10:59:15,408 - distributed.worker - INFO -       Start worker at:     tcp://172.26.1.2:43933
2025-02-08 10:59:15,408 - distributed.worker - INFO -          Listening to:     tcp://172.26.1.2:43933
2025-02-08 10:59:15,408 - distributed.worker - INFO -           Worker name:           SLURMCluster-4-4
2025-02-08 10:59:15,408 - distributed.worker - INFO -          dashboard at:           172.26.1.2:35655
2025-02-08 10:59:15,408 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:15,408 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,408 - distributed.worker - INFO -          dashboard at:           172.26.1.2:45077
2025-02-08 10:59:15,408 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:15,409 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,409 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:15,408 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:15,409 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:15,409 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:15,409 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6ot3w7m9
2025-02-08 10:59:15,409 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ga76eil3
2025-02-08 10:59:15,409 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,410 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,414 - distributed.worker - INFO -       Start worker at:     tcp://172.26.1.2:42503
2025-02-08 10:59:15,414 - distributed.worker - INFO -          Listening to:     tcp://172.26.1.2:42503
2025-02-08 10:59:15,414 - distributed.worker - INFO -           Worker name:           SLURMCluster-4-1
2025-02-08 10:59:15,415 - distributed.worker - INFO -          dashboard at:           172.26.1.2:41083
2025-02-08 10:59:15,415 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:15,415 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,415 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:15,415 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:15,415 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dpa3zifn
2025-02-08 10:59:15,415 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,519 - distributed.worker - INFO -       Start worker at:     tcp://172.26.1.2:38485
2025-02-08 10:59:15,519 - distributed.worker - INFO -          Listening to:     tcp://172.26.1.2:38485
2025-02-08 10:59:15,519 - distributed.worker - INFO -           Worker name:           SLURMCluster-4-3
2025-02-08 10:59:15,519 - distributed.worker - INFO -          dashboard at:           172.26.1.2:33463
2025-02-08 10:59:15,519 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:15,519 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,519 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:15,519 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:15,519 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_a1zk7_z
2025-02-08 10:59:15,519 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,243 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:16,244 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,244 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,244 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
2025-02-08 10:59:16,244 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:16,245 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:16,245 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,245 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,245 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,245 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,246 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:16,246 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
2025-02-08 10:59:16,246 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,246 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,247 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:16,248 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,248 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,248 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
2025-02-08 10:59:16,249 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
2025-02-08 10:59:16,249 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
slurmstepd-node02: error: *** JOB 54876 ON node02 CANCELLED AT 2025-02-08T11:14:38 DUE TO TIME LIMIT ***
