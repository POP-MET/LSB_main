2025-02-08 10:59:13,448 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.10:32833'
2025-02-08 10:59:13,451 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.10:43607'
2025-02-08 10:59:13,460 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.10:35591'
2025-02-08 10:59:13,470 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.10:40927'
2025-02-08 10:59:13,486 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.26.1.10:46525'
2025-02-08 10:59:15,237 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.10:45321
2025-02-08 10:59:15,237 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.10:45321
2025-02-08 10:59:15,238 - distributed.worker - INFO -           Worker name:           SLURMCluster-9-1
2025-02-08 10:59:15,238 - distributed.worker - INFO -          dashboard at:          172.26.1.10:44965
2025-02-08 10:59:15,238 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:15,238 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,238 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:15,238 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:15,238 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vw7749x9
2025-02-08 10:59:15,238 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,267 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.10:33117
2025-02-08 10:59:15,267 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.10:33117
2025-02-08 10:59:15,267 - distributed.worker - INFO -           Worker name:           SLURMCluster-9-3
2025-02-08 10:59:15,267 - distributed.worker - INFO -          dashboard at:          172.26.1.10:46085
2025-02-08 10:59:15,267 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:15,267 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,267 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:15,267 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:15,267 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xsw_mpmz
2025-02-08 10:59:15,267 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,554 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.10:46075
2025-02-08 10:59:15,555 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.10:46075
2025-02-08 10:59:15,555 - distributed.worker - INFO -           Worker name:           SLURMCluster-9-0
2025-02-08 10:59:15,555 - distributed.worker - INFO -          dashboard at:          172.26.1.10:46599
2025-02-08 10:59:15,555 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:15,555 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,555 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:15,555 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:15,555 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_8cltwls
2025-02-08 10:59:15,555 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,627 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.10:40625
2025-02-08 10:59:15,628 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.10:40625
2025-02-08 10:59:15,628 - distributed.worker - INFO -           Worker name:           SLURMCluster-9-2
2025-02-08 10:59:15,629 - distributed.worker - INFO -          dashboard at:          172.26.1.10:39429
2025-02-08 10:59:15,629 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:15,630 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,630 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:15,630 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:15,630 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-87h8ojn6
2025-02-08 10:59:15,630 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,634 - distributed.worker - INFO -       Start worker at:    tcp://172.26.1.10:45497
2025-02-08 10:59:15,634 - distributed.worker - INFO -          Listening to:    tcp://172.26.1.10:45497
2025-02-08 10:59:15,634 - distributed.worker - INFO -           Worker name:           SLURMCluster-9-4
2025-02-08 10:59:15,634 - distributed.worker - INFO -          dashboard at:          172.26.1.10:36281
2025-02-08 10:59:15,642 - distributed.worker - INFO - Waiting to connect to:   tcp://10.42.239.61:38535
2025-02-08 10:59:15,642 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:15,642 - distributed.worker - INFO -               Threads:                          2
2025-02-08 10:59:15,642 - distributed.worker - INFO -                Memory:                  18.63 GiB
2025-02-08 10:59:15,642 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kxut_6y0
2025-02-08 10:59:15,642 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,041 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:16,042 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,042 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,055 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
2025-02-08 10:59:16,094 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:16,095 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,095 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,104 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
2025-02-08 10:59:16,184 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:16,185 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,185 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,185 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
2025-02-08 10:59:16,186 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:16,187 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,187 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,188 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
2025-02-08 10:59:16,232 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-02-08 10:59:16,233 - distributed.worker - INFO -         Registered to:   tcp://10.42.239.61:38535
2025-02-08 10:59:16,233 - distributed.worker - INFO - -------------------------------------------------
2025-02-08 10:59:16,233 - distributed.core - INFO - Starting established connection to tcp://10.42.239.61:38535
slurmstepd-node10: error: *** JOB 54879 ON node10 CANCELLED AT 2025-02-08T11:14:38 DUE TO TIME LIMIT ***
